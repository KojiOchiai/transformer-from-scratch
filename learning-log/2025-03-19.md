# 2025-03-19 log

## 疑問点
- maskはどうやって決めている？
- datasetはどこからダウンロードしてる？
    - HuggingFace Dataset に自然言語処理などのデータセットが公開されている
    - datasetsというライブラリを使って取得できる
    - [HuggingFace Datasets の使い方｜npaka](https://note.com/npaka/n/n23b84c95faca)

## 実装中思ったことメモ
- blog記事ではソースコード表示の幅が狭いのでコードを写経するときは下記のレポジトリを見たほうがいい
    - [ES7/Transformer-from-Scratch: In this repository, I have explained the working of the Transformer architecture, provided the code for building it from scratch, and demonstrated how to train it.](https://github.com/ES7/Transformer-from-Scratch/tree/main)
- typehintのためにdatasetsとtokenizerをインストールした
    - どちらもHuggingfaceの出しているライブラリ
    - type hintに対応していないらしく # type: ignoreで対応した
- configとかdatasetの返り値とかdataclassやpydanticで定義したいけど、一旦我慢してお手本通りに作る
- type hint をつけながら写経するのは学習にいい
    - typeを意識することでコードの理解が進む
- importの自動ソートは写経のためには地味に不便
    - どこまで書いたかわからなくなる

## Todo
### [Building a Transformer from Scratch: A Step-by-Step Guide | by Ebad Sayed | Medium](https://medium.com/@sayedebad.777/building-a-transformer-from-scratch-a-step-by-step-guide-a3df0aeb7c9a)
- [x] InputEmbeddings
- [x] PositionalEncoding
- [x] LayerNormalization
- [x] FeedForwardBlock
- [x] MultiHeadAttentionBlock
- [x] ResidualConnection
- [x] EncoderBlock
- [x] Encoder
- [x] DecoderBlock
- [x] Decoder
- [x] ProjectionLayer
- [x] Transformer
- [x] buld_transformer

### [Training a Transformer Model from Scratch | by Ebad Sayed | Medium](https://medium.com/@sayedebad.777/training-a-transformer-model-from-scratch-25bb270f5888)
- [x] TranslationDataset
- [x] cofiguration
- [ ] training