# 2025-03-18 log

## 実装中思ったことメモ
### model
- Transformerは一見複雑そうに見えるが、整理していくと階層的な実装が可能で、各ステップごとの認知コストは低い
- 低層から順に理解していったおかげで、これまであった言語化しづらい気持ち悪さ（ブラックボックス感）は和らいだ
    - 脳内にマップができた = 変更したい時、どのあたりをいじればいいかイメージできるようになった
- そこそこ複雑なモジュールを組み合わせて、階層化するので、見るからに計算コストが高そうとわかる
- pytorchの可視化は分かりやすい。デバックで助かる
```
Transformer(
  (encoder): Encoder(
    (layers): ModuleList(
      (0-5): 6 x EncoderBlock(
        (self_attention_block): MultiHeadAttentionBlock(
          (w_q): Linear(in_features=512, out_features=512, bias=False)
          (w_k): Linear(in_features=512, out_features=512, bias=False)
          (w_v): Linear(in_features=512, out_features=512, bias=False)
          (w_o): Linear(in_features=512, out_features=512, bias=False)
          (droptout): Dropout(p=0.1, inplace=False)
        )
        (feed_forward_block): FeedForwardBlock(
          (linear_1): Linear(in_features=512, out_features=2048, bias=True)
          (droptou): Dropout(p=0.1, inplace=False)
          (linear_2): Linear(in_features=2048, out_features=512, bias=True)
        )
        (residual_connection): ModuleList(
          (0-1): 2 x ResidualConnection(
            (dropout): Dropout(p=0.1, inplace=False)
            (norm): LayerNormalization()
          )
        )
      )
    )
    (norm): LayerNormalization()
  )
  (decoder): Decoder(
    (layers): ModuleList(
      (0-5): 6 x DecoderBlock(
        (self_attention_block): MultiHeadAttentionBlock(
          (w_q): Linear(in_features=512, out_features=512, bias=False)
          (w_k): Linear(in_features=512, out_features=512, bias=False)
          (w_v): Linear(in_features=512, out_features=512, bias=False)
          (w_o): Linear(in_features=512, out_features=512, bias=False)
          (droptout): Dropout(p=0.1, inplace=False)
        )
        (cross_attention_block): MultiHeadAttentionBlock(
          (w_q): Linear(in_features=512, out_features=512, bias=False)
          (w_k): Linear(in_features=512, out_features=512, bias=False)
          (w_v): Linear(in_features=512, out_features=512, bias=False)
          (w_o): Linear(in_features=512, out_features=512, bias=False)
          (droptout): Dropout(p=0.1, inplace=False)
        )
        (feed_forward_block): FeedForwardBlock(
          (linear_1): Linear(in_features=512, out_features=2048, bias=True)
          (droptou): Dropout(p=0.1, inplace=False)
          (linear_2): Linear(in_features=2048, out_features=512, bias=True)
        )
        (residual_connections): ModuleList(
          (0-2): 3 x ResidualConnection(
            (dropout): Dropout(p=0.1, inplace=False)
            (norm): LayerNormalization()
          )
        )
      )
    )
    (norm): LayerNormalization()
  )
  (src_embed): InputEmbedding(
    (embedding): Embedding(100, 512)
  )
  (tgt_embed): InputEmbedding(
    (embedding): Embedding(100, 512)
  )
  (src_pos): PositionalEncoding(
    (dropout): Dropout(p=0.1, inplace=False)
  )
  (tgt_pos): PositionalEncoding(
    (dropout): Dropout(p=0.1, inplace=False)
  )
  (projection_layer): ProjectionLayer(
    (proj): Linear(in_features=512, out_features=100, bias=True)
  )
)
```

## Todo
### [Building a Transformer from Scratch: A Step-by-Step Guide | by Ebad Sayed | Medium](https://medium.com/@sayedebad.777/building-a-transformer-from-scratch-a-step-by-step-guide-a3df0aeb7c9a)
- [x] InputEmbeddings
- [x] PositionalEncoding
- [x] LayerNormalization
- [x] FeedForwardBlock
- [x] MultiHeadAttentionBlock
- [x] ResidualConnection
- [x] EncoderBlock
- [x] Encoder
- [x] DecoderBlock
- [x] Decoder
- [x] ProjectionLayer
- [x] Transformer
- [x] buld_transformer

### [Training a Transformer Model from Scratch | by Ebad Sayed | Medium](https://medium.com/@sayedebad.777/training-a-transformer-model-from-scratch-25bb270f5888)
- [ ] TranslationDataset
- [ ] cofiguation
- [ ] training