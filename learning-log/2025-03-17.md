# 2025-03-17 log

## 実装中気になったことメモ
- 引き続き写経しながら勉強を進める
- 基本教科書のとおりに書くが、型が分かりやすいようtype hintは追加する
### model
#### PositionalEncoding
- 参考：[Positional Encodingを理解したい #DeepLearning - Qiita](https://qiita.com/snsk871/items/93aba7ad74cace4abc62)
#### MultiHeadAttentionBlock
- view(): 配列の形を変形する。numpyのreshapeに相当。-1を指定すると次元サイズが自動的に計算される。
- contiguous(): メモリレイアウトを変えて、メモリ上で連続に配置する。これをやらないとviewでエラーになる可能性がある
- 参考: [マルチヘッドアテンション (Multi-head Attention) [Transformerの部品] | CVMLエキスパートガイド](https://cvml-expertguide.net/terms/dl/seq2seq-translation/transformer/multi-head-attention/)
- attensionがstaticmethodになっているのは、この処理中でselfにアクセスしないから（引数だけで計算ができるから）
#### ResidualConnection
- forwardでlayerを受け取ってそれを適用している
    - そういうのもありなんだなー

## 残り
### [Building a Transformer from Scratch: A Step-by-Step Guide | by Ebad Sayed | Medium](https://medium.com/@sayedebad.777/building-a-transformer-from-scratch-a-step-by-step-guide-a3df0aeb7c9a)
- [ ] EncoderBlock
- [ ] Encoder
- [ ] DecoderBlock
- [ ] Decoder
- [ ] ProjectionLayer
- [ ] Transformer
- [ ] buld_transformer

### [Training a Transformer Model from Scratch | by Ebad Sayed | Medium](https://medium.com/@sayedebad.777/training-a-transformer-model-from-scratch-25bb270f5888)
- [ ] TranslationDataset
- [ ] cofiguation
- [ ] training